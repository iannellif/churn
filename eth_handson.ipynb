{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iannellif/churn/blob/main/eth_handson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e54cd59d",
      "metadata": {
        "id": "e54cd59d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb6c5fc9",
      "metadata": {
        "id": "cb6c5fc9"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import tarfile\n",
        "import io\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/iannellif/churn/main/eth_df_churn.tar.gz\"\n",
        "\n",
        "# Download the tar.gz file\n",
        "response = requests.get(url)\n",
        "tar_file = tarfile.open(fileobj=io.BytesIO(response.content), mode=\"r:gz\")\n",
        "\n",
        "# Extract the CSV file\n",
        "tar_file.extractall(\"/content/churn_data\")\n",
        "tar_file.close()\n",
        "\n",
        "# List extracted files\n",
        "extracted_files = os.listdir(\"/content/churn_data\")\n",
        "print(\"Extracted files:\", extracted_files)\n",
        "\n",
        "df = pd.read_csv(\"/content/churn_data/eth_df_churn.csv\")\n",
        "\n",
        "print(\"\\nData loaded successfully. Shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "-WE0_QsY1Rij"
      },
      "id": "-WE0_QsY1Rij",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c95933a2-9d01-43b6-9946-af06424cd290",
      "metadata": {
        "id": "c95933a2-9d01-43b6-9946-af06424cd290"
      },
      "outputs": [],
      "source": [
        "# Define the disengagement threshold for the label\n",
        "decrease_percent = 0.25\n",
        "\n",
        "# Function to set negative values to 0\n",
        "def set_negative_to_zero(value):\n",
        "    return max(value, 0)\n",
        "\n",
        "# Apply the function to set negative values to 0 in '0Q' and '1Q'\n",
        "df['0Q'] = df['0Q'].apply(set_negative_to_zero)\n",
        "df['1Q'] = df['1Q'].apply(set_negative_to_zero)\n",
        "\n",
        "# Create the flag column based on the decrease percentage\n",
        "def calculate_flag(current, previous):\n",
        "    if previous == 0:\n",
        "        return 0  # Avoid division by zero\n",
        "    # percentage change between the current value and the previous value\n",
        "    change = (current - previous) / previous\n",
        "    return 1 if change < -decrease_percent else 0\n",
        "\n",
        "df['0Q_FLAG'] = df.apply(lambda row: calculate_flag(row['0Q'], row['-4Q']), axis=1)\n",
        "df['1Q_FLAG'] = df.apply(lambda row: calculate_flag(row['1Q'], row['-3Q']), axis=1)\n",
        "\n",
        "\n",
        "def categorize(row):\n",
        "    if row['0Q_FLAG'] == 0:\n",
        "        return 'NEGATIVE'\n",
        "    elif row['0Q_FLAG'] == 1 and row['1Q_FLAG'] == 1:\n",
        "        return 'TRUE POSITIVE'\n",
        "    elif row['0Q_FLAG'] == 1 and row['1Q_FLAG'] == 0:\n",
        "        return 'FALSE POSITIVE'\n",
        "    else:\n",
        "        return 'UNKNOWN'\n",
        "\n",
        "df['CAT'] = df.apply(categorize, axis=1)\n",
        "\n",
        "# Remove future flags and spending\n",
        "del df['1Q']\n",
        "del df['0Q_FLAG']\n",
        "del df['1Q_FLAG']\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['CAT'].value_counts()"
      ],
      "metadata": {
        "id": "uyGTGPFiLhMt"
      },
      "id": "uyGTGPFiLhMt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "192355b0-2068-4280-a9a6-9729d1b7074b",
      "metadata": {
        "id": "192355b0-2068-4280-a9a6-9729d1b7074b"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "135e6897-c1cd-470a-8968-9fce29ec7321",
      "metadata": {
        "id": "135e6897-c1cd-470a-8968-9fce29ec7321"
      },
      "outputs": [],
      "source": [
        "(df.isna().sum()/len(df)*100).plot(kind = 'barh', figsize = (10,10))\n",
        "plt.title('Percentage of Missing Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f174f6a8-51e3-471e-bf5a-cb4ce2c1cd72",
      "metadata": {
        "id": "f174f6a8-51e3-471e-bf5a-cb4ce2c1cd72"
      },
      "outputs": [],
      "source": [
        "df.fillna(0, inplace=True)\n",
        "\n",
        "(df.isna().sum()/len(df)*100).plot(kind = 'barh', figsize = (10,10))\n",
        "plt.title('Percentage of Missing Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18f91a93",
      "metadata": {
        "id": "18f91a93"
      },
      "outputs": [],
      "source": [
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "# skip the label columns\n",
        "categorical_cols = categorical_cols.drop(['CAT'])\n",
        "categorical_cols = categorical_cols.tolist()\n",
        "categorical_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "768d019d-0a24-4272-b83b-5c7b1969229c",
      "metadata": {
        "id": "768d019d-0a24-4272-b83b-5c7b1969229c"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f176f59a",
      "metadata": {
        "id": "f176f59a"
      },
      "outputs": [],
      "source": [
        "# One-hot encoding\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded_cols = encoder.fit_transform(df[categorical_cols])\n",
        "encoded_cols = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "encoded_cols.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3217fdb4",
      "metadata": {
        "id": "3217fdb4"
      },
      "outputs": [],
      "source": [
        "# Dropping original categorical columns and adding encoded ones\n",
        "dffdrop = df.drop(categorical_cols, axis=1)\n",
        "df = pd.concat([dffdrop, encoded_cols], axis=1)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83106822",
      "metadata": {
        "id": "83106822"
      },
      "source": [
        "# Features and Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0314ac31",
      "metadata": {
        "id": "0314ac31"
      },
      "outputs": [],
      "source": [
        "df_model = df.copy()\n",
        "\n",
        "labels = df_model['CAT'].map({'TRUE POSITIVE': 1, 'FALSE POSITIVE': 0, 'NEGATIVE' : 0})\n",
        "\n",
        "features = df_model.drop(columns=['CAT']).iloc[:, np.r_[0:27]]\n",
        "# features = df_model.drop(columns=['CAT']).iloc[:, np.r_[0:27,51:len(df_model.columns)-1]]\n",
        "\n",
        "print(features.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "371b09c4-665e-498d-92f7-c21dfa87c283",
      "metadata": {
        "id": "371b09c4-665e-498d-92f7-c21dfa87c283"
      },
      "outputs": [],
      "source": [
        "# Compute the correlation matrix\n",
        "corr = features.corr()\n",
        "\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "sns.heatmap(corr, mask=mask, cmap='viridis', vmax=1., vmin=-1, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .8})\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3fd731a-8df3-4067-ac0e-b0e73b4f3eb2",
      "metadata": {
        "id": "c3fd731a-8df3-4067-ac0e-b0e73b4f3eb2"
      },
      "outputs": [],
      "source": [
        "labels.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b71209a-5903-479e-b80f-08cc89f7322b",
      "metadata": {
        "id": "0b71209a-5903-479e-b80f-08cc89f7322b"
      },
      "outputs": [],
      "source": [
        "tags = 'NEGATIVE', 'POSITIVE'\n",
        "sizes = [labels[labels==0].count(), labels[labels==1].count()]\n",
        "explode = (0, 0.1)\n",
        "fig1, ax1 = plt.subplots(figsize=(6, 4))\n",
        "ax1.pie(sizes, explode=explode, labels=tags, autopct='%1.1f%%',\n",
        "        shadow=True, startangle=90)\n",
        "ax1.axis('equal')\n",
        "plt.title(\"Churn Ratio\", size = 20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tags = ['NEGATIVE', 'TRUE POSITIVE', 'FALSE POSITIVE']\n",
        "sizes = [df['CAT'][df['CAT'] == 'NEGATIVE'].count(),\n",
        "         df['CAT'][df['CAT'] == 'TRUE POSITIVE'].count(),\n",
        "         df['CAT'][df['CAT'] == 'FALSE POSITIVE'].count()]\n",
        "explode = (0, 0.1, 0.1)\n",
        "\n",
        "fig1, ax1 = plt.subplots(figsize=(6, 4))\n",
        "ax1.pie(sizes, explode=explode, labels=tags, autopct='%1.1f%%',\n",
        "        shadow=True, startangle=90)\n",
        "ax1.axis('equal')\n",
        "plt.title(\"Churn Category Distribution\", size=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U-ScxpIAMJrq"
      },
      "id": "U-ScxpIAMJrq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5b1d10a1",
      "metadata": {
        "id": "5b1d10a1"
      },
      "source": [
        "# Train data split and scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0fbdf77-f070-4ab7-895a-7e794f20c37c",
      "metadata": {
        "id": "b0fbdf77-f070-4ab7-895a-7e794f20c37c"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dTg_-D3Yx4QM",
      "metadata": {
        "id": "dTg_-D3Yx4QM"
      },
      "source": [
        "Sample division into train, validation, and test datasets. There is no predefined rule; it largely depends on the total dataset size. Typically, it’s 60:20:20, but with large datasets, it can go up to 98:1:1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a97722e",
      "metadata": {
        "id": "1a97722e"
      },
      "outputs": [],
      "source": [
        "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78193fdc-1bd1-4776-9e12-622b147a5f8a",
      "metadata": {
        "id": "78193fdc-1bd1-4776-9e12-622b147a5f8a"
      },
      "outputs": [],
      "source": [
        "print('Training data shape: ',features_train.shape)\n",
        "print('Training targets shape: ',labels_train.shape)\n",
        "print('Test data shape: ',features_test.shape)\n",
        "print('Test targets shape: ',labels_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "764091df-5b08-41bc-812c-3221dd5373c0",
      "metadata": {
        "id": "764091df-5b08-41bc-812c-3221dd5373c0"
      },
      "source": [
        "# ML Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfe14352-550e-4708-ae3a-5922c1b8885f",
      "metadata": {
        "id": "dfe14352-550e-4708-ae3a-5922c1b8885f"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53403f47-29d7-4aca-94b3-713e3662e906",
      "metadata": {
        "id": "53403f47-29d7-4aca-94b3-713e3662e906"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59b11653-1ffe-4320-9a1a-3e3956f53ddd",
      "metadata": {
        "id": "59b11653-1ffe-4320-9a1a-3e3956f53ddd"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c11e66-180f-4562-8186-4e361a0ac798",
      "metadata": {
        "id": "a1c11e66-180f-4562-8186-4e361a0ac798"
      },
      "outputs": [],
      "source": [
        "# Define the pipelines\n",
        "pipelines = {\n",
        "    'nb': Pipeline([('s', StandardScaler()),\n",
        "                    ('nb', GaussianNB())]),\n",
        "    'dt': Pipeline([('s', StandardScaler()),\n",
        "                    ('dt', DecisionTreeClassifier())]),\n",
        "    # 'rf': Pipeline([('s', StandardScaler()),\n",
        "    #                 ('rf', RandomForestClassifier())]),\n",
        "    'lb': Pipeline([('s', StandardScaler()),\n",
        "                    ('lb', LGBMClassifier())])\n",
        "}\n",
        "\n",
        "\n",
        "print('# Fit the models')\n",
        "for model in pipelines.values():\n",
        "    print(model)\n",
        "    model.fit(features_train, labels_train)\n",
        "\n",
        "print('# Predict and evaluate')\n",
        "for name, model in pipelines.items():\n",
        "    y_pred = model.predict(features_test)\n",
        "\n",
        "    print(classification_report(labels_test, y_pred))\n",
        "\n",
        "    accuracy = accuracy_score(labels_test, y_pred)\n",
        "\n",
        "    print(f\"Model: {name}, Accuracy: {accuracy}\")\n",
        "\n",
        "    print('Recall:', recall_score(labels_test, y_pred, average=None))\n",
        "\n",
        "    print('Precision:', precision_score(labels_test, y_pred, average=None))\n",
        "\n",
        "    cm = confusion_matrix(labels_test, y_pred)\n",
        "    print('Confusion Matrix:\\n', cm)\n",
        "    ConfusionMatrixDisplay(confusion_matrix=cm).plot(values_format='')\n",
        "    plt.show()\n",
        "    # cm = confusion_matrix(labels_test, y_pred,normalize = 'pred')\n",
        "    # ConfusionMatrixDisplay(confusion_matrix=cm).plot(values_format='.0%')\n",
        "    # plt.show()\n",
        "    # cm = confusion_matrix(labels_test, y_pred,normalize = 'true')\n",
        "    # ConfusionMatrixDisplay(confusion_matrix=cm).plot(values_format='.0%')\n",
        "    # plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92ff0088-20db-43af-9cee-88773168a9a2",
      "metadata": {
        "id": "92ff0088-20db-43af-9cee-88773168a9a2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc, precision_recall_curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90a3a09d-5b1a-477d-9caf-60b535269cd9",
      "metadata": {
        "id": "90a3a09d-5b1a-477d-9caf-60b535269cd9"
      },
      "outputs": [],
      "source": [
        "# Plot combined ROC curves\n",
        "plt.figure(figsize=(6, 3))\n",
        "for name, model in pipelines.items():\n",
        "    # Calculate ROC-AUC and plot ROC curve\n",
        "    if hasattr(model.named_steps[model.steps[-1][0]], \"predict_proba\"):\n",
        "        y_prob = model.predict_proba(features_test)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(labels_test, y_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, lw=2, label=f'{name} (area = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic - All Models')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Plot combined precision-recall curves\n",
        "plt.figure(figsize=(6, 3))\n",
        "for name, model in pipelines.items():\n",
        "    if hasattr(model.named_steps[model.steps[-1][0]], \"predict_proba\"):\n",
        "        y_prob = model.predict_proba(features_test)[:, 1]\n",
        "        precision, recall, _ = precision_recall_curve(labels_test, y_prob)\n",
        "        pr_auc = auc(recall, precision)\n",
        "\n",
        "        plt.plot(recall, precision, lw=2, label=f'{name} (area = {pr_auc:.2f})')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve - All Models')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b623a6d-85ef-46e3-8518-0f0da5c53532",
      "metadata": {
        "id": "7b623a6d-85ef-46e3-8518-0f0da5c53532"
      },
      "source": [
        "# PyTorch ANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "138135e0-e7cd-4a3a-80f5-30ae5bad9462",
      "metadata": {
        "id": "138135e0-e7cd-4a3a-80f5-30ae5bad9462"
      },
      "outputs": [],
      "source": [
        "# pytorch\n",
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2bd4321-07de-49a0-bf1b-bcf082cb5d38",
      "metadata": {
        "id": "e2bd4321-07de-49a0-bf1b-bcf082cb5d38"
      },
      "source": [
        "Using the GPU. If a GPU is available, it can be used to accelerate operations. This requires moving tensors to the GPU when performing calculations. It is advisable to check if the GPU is available and, if so, set an appropriate variable for subsequent use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd7d4adf",
      "metadata": {
        "id": "cd7d4adf"
      },
      "outputs": [],
      "source": [
        "# let's check if GPU is available and which type\n",
        "if torch.cuda.is_available():\n",
        "  print('Available GPUs number: ',torch.cuda.device_count())\n",
        "  for i in range(0,torch.cuda.device_count()):\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "\n",
        "# if GPU is avialable let's set device='cuda', else 'cpu\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Computation device: {device}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d375f54-3857-4731-91b1-6141c1077a92",
      "metadata": {
        "id": "1d375f54-3857-4731-91b1-6141c1077a92"
      },
      "source": [
        "## Data Preprocessing and Conversion to PyTorch Tensors\n",
        "\n",
        "This code cell performs several key preprocessing steps on the dataset:\n",
        "\n",
        "1. **Standardization**: A `StandardScaler` object is created to standardize the features by removing the mean and scaling to unit variance. The scaler is fitted on the training data and then used to transform both the training and test datasets.\n",
        "2. **Conversion to PyTorch Tensors**: The standardized features and labels from the pandas dataframes are converted into PyTorch tensors, which are required for training a neural network using PyTorch.\n",
        "\n",
        "These steps ensure that the data is properly scaled and in the correct format for model training and evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bf252b4",
      "metadata": {
        "id": "5bf252b4"
      },
      "outputs": [],
      "source": [
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit on your training data and transform both train and test data\n",
        "features_train = scaler.fit_transform(features_train)\n",
        "features_test = scaler.transform(features_test)\n",
        "\n",
        "# Convert the pandas dataframes into PyTorch tensors\n",
        "train_data = torch.tensor(features_train, dtype=torch.float)\n",
        "train_labels = torch.tensor(labels_train.to_numpy(), dtype=torch.long)\n",
        "\n",
        "test_data = torch.tensor(features_test, dtype=torch.float)\n",
        "test_labels = torch.tensor(labels_test.to_numpy(), dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48b64908-d0de-48a7-aff0-513dd5519302",
      "metadata": {
        "id": "48b64908-d0de-48a7-aff0-513dd5519302"
      },
      "source": [
        "NOTE: Normalization of input features: Normalizing the input allows for the use of larger learning rates (faster training) and stabilizes the training process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a392d9c-be8f-44d3-b3ae-db5d30c32d02",
      "metadata": {
        "id": "2a392d9c-be8f-44d3-b3ae-db5d30c32d02"
      },
      "source": [
        "# Neural network model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69c2f7bc-6f2c-45a5-8e36-fcddc374a064",
      "metadata": {
        "id": "69c2f7bc-6f2c-45a5-8e36-fcddc374a064"
      },
      "source": [
        "## Shallow MLP\n",
        "\n",
        "Architecture: Shallow MLP with dense layers (Linear in PyTorch): With a single hidden layer using ReLU activations, and 2 output neurons with softmax activation predicting the probability that the customer churned or not:\n",
        "\n",
        "*   input layer: #features neurons\n",
        "*   hidden layer: 1024 neurons, ReLU activation\n",
        "*   output layer: 2 neurons, softmax activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b430f2d8",
      "metadata": {
        "id": "b430f2d8"
      },
      "outputs": [],
      "source": [
        "inputneurons = features.shape[1]\n",
        "inputneurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfbfd224",
      "metadata": {
        "id": "cfbfd224"
      },
      "outputs": [],
      "source": [
        "# define the 1st layer neurons\n",
        "layer1neurons = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6b4791f",
      "metadata": {
        "id": "b6b4791f"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "# define the class CustomDataset that inherits from PyTorch’s Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    # constructor method that initializes the dataset object\n",
        "    def __init__(self, features, labels):\n",
        "        # converts the features input into a PyTorch tensor of type float\n",
        "        self.data = torch.tensor(features, dtype=torch.float)\n",
        "        # converts the labels input into a PyTorch tensor of type long\n",
        "        self.targets = torch.tensor(labels.to_numpy(), dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        # returns the number of samples in the dataset\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # retrieves the feature and target at the specified index idx\n",
        "        return self.data[idx], self.targets[idx]\n",
        "\n",
        "# Create your datasets:\n",
        "# train_data and test_data are instances of the CustomDataset class,\n",
        "# created using training and testing features and labels\n",
        "train_data = CustomDataset(features_train, labels_train)\n",
        "test_data = CustomDataset(features_test, labels_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc1f7568-070b-4dae-aa11-fa0c5fb27e60",
      "metadata": {
        "id": "cc1f7568-070b-4dae-aa11-fa0c5fb27e60"
      },
      "source": [
        "This custom dataset can now be used with PyTorch’s data loaders to efficiently load data during model training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdb4c0e7",
      "metadata": {
        "id": "bdb4c0e7"
      },
      "outputs": [],
      "source": [
        "print('Training data shape: ',train_data.data.shape)\n",
        "print('Training targets shape: ',train_data.targets.shape)\n",
        "print('Test data shape: ',test_data.data.shape)\n",
        "print('Test targets shape: ',test_data.targets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2528a58",
      "metadata": {
        "id": "d2528a58"
      },
      "outputs": [],
      "source": [
        "# first customer features and label of the training sample\n",
        "customer = (train_data.data)[0]\n",
        "label = (train_data.targets)[0]\n",
        "\n",
        "print(type(customer)) # tensor torch\n",
        "print(type(label)) # tensor torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d4de34a-afd7-4d5e-bc09-2c331aaf9c60",
      "metadata": {
        "id": "7d4de34a-afd7-4d5e-bc09-2c331aaf9c60"
      },
      "outputs": [],
      "source": [
        "print('shape customer: ', customer.shape)\n",
        "print('shape customer: ', customer.numpy().shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25ed8843-1067-4a3d-9f69-28463fce5747",
      "metadata": {
        "id": "25ed8843-1067-4a3d-9f69-28463fce5747"
      },
      "outputs": [],
      "source": [
        "print(customer)\n",
        "print(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55ff6d35-dd55-4bbd-9ab5-86fed078657f",
      "metadata": {
        "id": "55ff6d35-dd55-4bbd-9ab5-86fed078657f"
      },
      "outputs": [],
      "source": [
        "print((train_data[0])[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63976591",
      "metadata": {
        "id": "63976591"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# In PyTorch, a neural network is implemented by creating a Python class that inherits\n",
        "# from PyTorch's nn.Module class and implements two basic methods:\n",
        "# __init__: definition of the layers used\n",
        "# forward: function that computes y = ANN(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e7ea525-f0bb-4dff-8f2a-62e3a946284c",
      "metadata": {
        "id": "4e7ea525-f0bb-4dff-8f2a-62e3a946284c"
      },
      "outputs": [],
      "source": [
        "class ShallowMLP(nn.Module):\n",
        "    def __init__(self, input_dim=inputneurons, output_dim=2, hidden_dim=layer1neurons):\n",
        "        super(ShallowMLP, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Linear(input_dim, hidden_dim)  #hidden layer\n",
        "        self.layer2 = nn.Linear(hidden_dim, output_dim) #output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = F.relu(x)\n",
        "        # x = F.tanh(x)\n",
        "#         x = F.sigmoid(x)\n",
        "        out = self.layer2(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a97d2d75",
      "metadata": {
        "id": "a97d2d75"
      },
      "outputs": [],
      "source": [
        "# we can do the same for a deep ANN\n",
        "class DeepMLP(nn.Module):\n",
        "    def __init__(self, input_dim=inputneurons, output_dim=2, hidden_dim1=layer1neurons, hidden_dim2=layer1neurons):\n",
        "        super(DeepMLP, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Linear(input_dim, hidden_dim1)  # first hidden layer\n",
        "        self.layer2 = nn.Linear(hidden_dim1, hidden_dim2)  # second hidden layer\n",
        "        self.layer3 = nn.Linear(hidden_dim2, output_dim)  # output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))  # activation function for first hidden layer\n",
        "        x = F.relu(self.layer2(x))  # activation function for second hidden layer\n",
        "        out = self.layer3(x)  # no activation function for the output layer\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad0a6506-5918-485d-8e07-84dbdef8fafb",
      "metadata": {
        "id": "ad0a6506-5918-485d-8e07-84dbdef8fafb"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bbdfe2f",
      "metadata": {
        "id": "5bbdfe2f"
      },
      "outputs": [],
      "source": [
        "# printout the model\n",
        "\n",
        "model = ShallowMLP()\n",
        "model = DeepMLP()\n",
        "print(model)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  summary(model.cuda(), input_size=(1,inputneurons))\n",
        "else:\n",
        "  summary(model, input_size=(1,inputneurons))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3988a75c-1ba0-42c1-bb0e-38670c40b0c6",
      "metadata": {
        "id": "3988a75c-1ba0-42c1-bb0e-38670c40b0c6"
      },
      "source": [
        "## Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1d3895a",
      "metadata": {
        "id": "b1d3895a"
      },
      "outputs": [],
      "source": [
        "# Train-Validation:\n",
        "# We split the dataset into two subsets to be used in training as\n",
        "# the training set and validation set, respectively. The validation set is used to tune\n",
        "# the network's hyperparameters and evaluate the model’s performance on unseen data.\n",
        "\n",
        "tr_data, val_data = torch.utils.data.random_split(train_data, [train_data.targets.shape[0] - test_data.targets.shape[0], test_data.targets.shape[0]])\n",
        "\n",
        "# Mini-Batches:\n",
        "# When training a neural network using SGD (Stochastic Gradient Descent), especially with large datasets,\n",
        "# it’s computationally efficient to divide the data into smaller chunks called mini-batches.\n",
        "# This allows the model to update its parameters more frequently and can lead to faster convergence.\n",
        "# This process is handled automatically in PyTorch through helper functions called Data Loaders.\n",
        "\n",
        "batch = 100\n",
        "\n",
        "# Data Loaders:\n",
        "# In PyTorch, DataLoader is a utility that abstracts the complexity of managing mini-batches,\n",
        "# reshuffling, and applying transformations. It provides an efficient way to iterate over the dataset.\n",
        "# Data transformations (like normalization, augmentation, etc.) are often applied to the training data\n",
        "# to improve the model’s performance and robustness.\n",
        "# To ensure that the model generalizes well and does not overfit to the order of the training data,\n",
        "# it’s common practice to reshuffle the data at the beginning of each epoch.\n",
        "# We use three loader (train, val and test) (or we could use a dictionary to group them and read them at once)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dl = DataLoader(tr_data,\n",
        "                      batch_size=batch,\n",
        "                      shuffle=True,\n",
        "                      num_workers=0,\n",
        "                      drop_last=True)\n",
        "\n",
        "vali_dl = DataLoader(val_data,\n",
        "                      batch_size=batch,\n",
        "                      shuffle=True,\n",
        "                      num_workers=0,\n",
        "                      drop_last=True)\n",
        "\n",
        "test_dl = DataLoader(test_data,\n",
        "                     batch_size=batch,\n",
        "                     shuffle=True,\n",
        "                     num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96234cbc",
      "metadata": {
        "id": "96234cbc"
      },
      "outputs": [],
      "source": [
        "# next(iter(train_dl)) retrieves the first batch of data from the train_dl data loader.\n",
        "# Let's test a batch on the 'untrained' model to check that everything works well\n",
        "\n",
        "feat, label = next(iter(train_dl))\n",
        "feat, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c755e273",
      "metadata": {
        "id": "c755e273"
      },
      "outputs": [],
      "source": [
        "feat.shape, label.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3da980b6-ba50-4d06-8b5d-8e0521137c5c",
      "metadata": {
        "id": "3da980b6-ba50-4d06-8b5d-8e0521137c5c"
      },
      "outputs": [],
      "source": [
        "# Move the feat (features) and label (labels) tensors to the specified device (e.g., CPU or GPU)\n",
        "# Moving tensors to the appropriate device ensures that computations are performed on the GPU if available,\n",
        "# which can significantly speed up training and inference.\n",
        "feat=feat.to(device)\n",
        "label=label.to(device)\n",
        "\n",
        "# Pass the features (feat) through the model to get the output (out)\n",
        "out = model(feat)\n",
        "\n",
        "# The shape of the prediction is a batch*classes_number tensor\n",
        "print(out.shape)\n",
        "\n",
        "# First element of the output tensor, i.e. the predictions for the first sample in the batch.\n",
        "# The values are the raw output (logits) from the model for the first sample in the batch\n",
        "print(out[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bdd9643-1bc7-4428-8bb6-6f69453ae069",
      "metadata": {
        "id": "4bdd9643-1bc7-4428-8bb6-6f69453ae069"
      },
      "outputs": [],
      "source": [
        "# The logits are not probabilities but can be converted to probabilities using a softmax function\n",
        "# This applies the softmax function along the second dimension (the class dimension) of the output tensor, converting the logits to probabilities\n",
        "probs = F.softmax(out, dim=1)\n",
        "print(probs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f3c4479",
      "metadata": {
        "id": "6f3c4479"
      },
      "outputs": [],
      "source": [
        "# To complete the model, we need to define the loss function, any metrics to monitor the\n",
        "# training of the network, and finally the optimizer\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss() #cross entropy loss\n",
        "# loss_func = nn.NLLLoss() #alternatively with log_softmax (same result)\n",
        "\n",
        "\n",
        "def accuracy(yhat, y):\n",
        "    #predictions == neurons with the highest probability\n",
        "    #computes the maximum value along dimension 1 (the class dimension in classification) for each sample in yhat\n",
        "    preds = torch.max(yhat,1)[1]\n",
        "\n",
        "    #count the number of True values, giving the total number of correct predictions in the batch\n",
        "    batch_acc = (preds == y).sum()\n",
        "    return batch_acc\n",
        "\n",
        "\n",
        "# !pip install torchmetrics\n",
        "\n",
        "# from torchmetrics.classification import MulticlassPrecision, MulticlassRecall\n",
        "\n",
        "# # Initialize metrics with the appropriate task\n",
        "# precision = MulticlassPrecision(num_classes=2)\n",
        "# recall = MulticlassRecall(num_classes=2)\n",
        "\n",
        "\n",
        "def precision(yhat, y):\n",
        "    preds = torch.max(yhat, 1)[1]\n",
        "    tp = torch.sum((preds == 1) & (y == 1)).float()\n",
        "    fp = torch.sum((preds == 1) & (y == 0)).float()\n",
        "\n",
        "    prec = tp / (tp + fp) if tp + fp > 0 else torch.tensor(0.0).to(y.device)\n",
        "    return prec\n",
        "\n",
        "def recall(yhat, y):\n",
        "    preds = torch.max(yhat, 1)[1]\n",
        "    tp = torch.sum((preds == 1) & (y == 1)).float()\n",
        "    fn = torch.sum((preds == 0) & (y == 1)).float()\n",
        "\n",
        "    rec = tp / (tp + fn) if tp + fn > 0 else torch.tensor(0.0).to(y.device)\n",
        "    return rec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "676c2ba2-931b-4d09-aeca-8dcff5904ef4",
      "metadata": {
        "id": "676c2ba2-931b-4d09-aeca-8dcff5904ef4"
      },
      "outputs": [],
      "source": [
        "# optimizer: for example, we use stochastic gradient descent with momentum,\n",
        "# that helps accelerate SGD in the relevant direction and dampens oscillations.\n",
        "# Momentum helps overcome small local minima. Without it, the optimizer might\n",
        "# get stuck more easily.\n",
        "# The weights and biases of the neural network that need to be updated\n",
        "# during training are defined by model.parameters() and the learning rate lr\n",
        "# which controls how much to change the model parameters in response\n",
        "# to the estimated error each time the model weights are updated\n",
        "\n",
        "from torch import optim\n",
        "opt = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "# opt = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bf98d9d",
      "metadata": {
        "id": "4bf98d9d"
      },
      "outputs": [],
      "source": [
        "# if available we move the model on the GPU\n",
        "model.to(device)\n",
        "print(next(model.parameters()).device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02SwQfLFKge2",
      "metadata": {
        "id": "02SwQfLFKge2"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# In pytorch we need to write the training loop, i.e. the loop on the epochs of training.\n",
        "# In each epoch we read all events of the dataset and update the nn weigths after each mini-batch\n",
        "# number of epochs\n",
        "epochs = 100\n",
        "\n",
        "# lists to save the value of the loss and the metrics at each epoch to be able to plot them as a function\n",
        "# of the epoch at the end of training\n",
        "hist_time = []\n",
        "hist_loss = []\n",
        "hist_accuracy = []\n",
        "hist_precision = []\n",
        "hist_recall = []\n",
        "hist_vloss = []\n",
        "hist_vaccuracy = []\n",
        "hist_vprecision = []\n",
        "hist_vrecall = []\n",
        "\n",
        "# loop over epochs\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch: {epoch + 1}\")\n",
        "    t0 = time.time()\n",
        "    # training step (where we update the weights of the neural network)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_accuracy = 0\n",
        "    train_precision = 0\n",
        "    train_recall = 0\n",
        "    counter = 0\n",
        "    for xb, yb in train_dl:\n",
        "        counter += 1\n",
        "        xb = xb.to(device)  # copy the mini-batch of data to the CPU/GPU\n",
        "        yb = yb.to(device)  # copy the mini-batch of labels to the CPU/GPU\n",
        "        pred = model(xb)  # model prediction\n",
        "        # calculate loss and metrics\n",
        "        loss = loss_func(pred, yb)\n",
        "        batch_accuracy = accuracy(pred, yb)\n",
        "        batch_precision = precision(pred, yb)\n",
        "        batch_recall = recall(pred, yb)\n",
        "        # update total loss and metrics\n",
        "        train_loss += loss.item()\n",
        "        train_accuracy += batch_accuracy.item()\n",
        "        train_precision += batch_precision.item()\n",
        "        train_recall += batch_recall.item()\n",
        "        # backpropagation\n",
        "        opt.zero_grad()  # reset gradients before performing backpropagation\n",
        "        loss.backward()  # calculate the gradients of the loss\n",
        "        opt.step()  # update the weights\n",
        "    train_loss /= counter\n",
        "    train_accuracy /= (counter * batch)\n",
        "    train_precision /= counter\n",
        "    train_recall /= counter\n",
        "    hist_loss.append(train_loss)\n",
        "    hist_accuracy.append(train_accuracy)\n",
        "    hist_precision.append(train_precision)\n",
        "    hist_recall.append(train_recall)\n",
        "\n",
        "    # validation step (weights are not updated)\n",
        "    model.eval() # ensures that the model behaves consistently during evaluation, giving the same output for the same input\n",
        "    vali_loss = 0\n",
        "    vali_accuracy = 0\n",
        "    vali_precision = 0\n",
        "    vali_recall = 0\n",
        "    counter = 0\n",
        "    with torch.no_grad():  # prevent weights from being updated\n",
        "        for xb, yb in vali_dl:\n",
        "            counter += 1\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            pred = model(xb)  # model prediction\n",
        "            # calculate loss and metrics\n",
        "            vloss = loss_func(pred, yb)\n",
        "            batch_accuracy = accuracy(pred, yb)\n",
        "            batch_precision = precision(pred, yb)\n",
        "            batch_recall = recall(pred, yb)\n",
        "            vali_loss += vloss.item()\n",
        "            vali_accuracy += batch_accuracy.item()\n",
        "            vali_precision += batch_precision.item()\n",
        "            vali_recall += batch_recall.item()\n",
        "    vali_loss /= counter\n",
        "    vali_accuracy /= (counter * batch)\n",
        "    vali_precision /= counter\n",
        "    vali_recall /= counter\n",
        "    hist_vloss.append(vali_loss)\n",
        "    hist_vaccuracy.append(vali_accuracy)\n",
        "    hist_vprecision.append(vali_precision)\n",
        "    hist_vrecall.append(vali_recall)\n",
        "\n",
        "    elapsed_time = time.time() - t0\n",
        "    hist_time.append(elapsed_time)\n",
        "    print(f\"Time: {elapsed_time:.4f}s\")\n",
        "    print(f\"Train -> Loss: {train_loss:.6f}, Accuracy: {train_accuracy:.6f}, Precision: {train_precision:.6f}, Recall: {train_recall:.6f}\")\n",
        "    print(f\"Val   -> Loss: {vali_loss:.6f}, Accuracy: {vali_accuracy:.6f}, Precision: {vali_precision:.6f}, Recall: {vali_recall:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab00ddea",
      "metadata": {
        "id": "ab00ddea"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(range(1,len(hist_loss)+1), hist_loss, color='blue', linestyle='-', label='training', lw=2)\n",
        "plt.plot(range(1,len(hist_vloss)+1), hist_vloss, color='green', linestyle='-', label='validation', lw=2)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(range(1,len(hist_accuracy)+1),hist_accuracy, color='blue', linestyle='-', label='training', lw=2)\n",
        "plt.plot(range(1,len(hist_vaccuracy)+1),hist_vaccuracy, color='green', linestyle='-', label='validation', lw=2)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(range(1,len(hist_precision)+1),hist_precision, color='blue', linestyle='-', label='training', lw=2)\n",
        "plt.plot(range(1,len(hist_vprecision)+1),hist_vprecision, color='green', linestyle='-', label='validation', lw=2)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(range(1,len(hist_recall)+1),hist_recall, color='blue', linestyle='-', label='training', lw=2)\n",
        "plt.plot(range(1,len(hist_vrecall)+1),hist_vrecall, color='green', linestyle='-', label='validation', lw=2)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Recall')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "226a9f51-6b9e-4aa6-aac4-0fa2e9c8e4d7",
      "metadata": {
        "id": "226a9f51-6b9e-4aa6-aac4-0fa2e9c8e4d7"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(range(1,len(hist_time)+1),hist_time, color='red', linestyle='-', label='training', lw=2)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('CPU Time')\n",
        "# plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f6c2613",
      "metadata": {
        "id": "0f6c2613"
      },
      "source": [
        "# Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "defb155e",
      "metadata": {
        "id": "defb155e"
      },
      "outputs": [],
      "source": [
        "# torch.save(model, 'torch_trained_eth.pt')\n",
        "# model = torch.load('./torch_trained_eth.pt')\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lW0Yuyhkvgig",
      "metadata": {
        "id": "lW0Yuyhkvgig"
      },
      "outputs": [],
      "source": [
        "# Some layers like dropout and batch normalization behave differently\n",
        "# during training and inference.\n",
        "# eval() ensures they behave correctly for inference\n",
        "model.eval()\n",
        "\n",
        "# for the inference it is not necessay to run on GPU\n",
        "model.to(torch.device('cpu'))\n",
        "\n",
        "# Initialize variables for metrics\n",
        "test_loss = 0\n",
        "test_accuracy = 0\n",
        "test_precision = 0\n",
        "test_recall = 0\n",
        "counter = 0\n",
        "\n",
        "# Initialize empty lists for true and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "with torch.no_grad():  # Disable gradient computation\n",
        "    for xb, yb in test_dl:\n",
        "        counter += 1\n",
        "        xb = xb.to('cpu')\n",
        "        yb = yb.to('cpu')\n",
        "        pred = model(xb)\n",
        "\n",
        "        # Calculate metrics\n",
        "        test_loss += loss_func(pred, yb).item()\n",
        "        test_accuracy += accuracy(pred, yb).item()\n",
        "        test_precision += precision(pred, yb).item()\n",
        "        test_recall += recall(pred, yb).item()\n",
        "\n",
        "        # Convert predicted labels to numpy array\n",
        "        pred_labels = torch.max(pred, 1)[1].numpy()\n",
        "        true_labels.extend(yb.numpy())\n",
        "        predicted_labels.extend(pred_labels)\n",
        "\n",
        "# Calculate average metrics\n",
        "test_loss /= counter\n",
        "test_accuracy /= (counter * batch)\n",
        "test_precision /= counter\n",
        "test_recall /= counter\n",
        "\n",
        "# Print results\n",
        "print(f'Test loss: {test_loss:.6f}')\n",
        "print(f'Test accuracy: {test_accuracy:.6f}')\n",
        "print(f'Test precision: {test_precision:.6f}')\n",
        "print(f'Test recall: {test_recall:.6f}')\n",
        "\n",
        "# Compute F1 score\n",
        "f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
        "print(f'Test F1 score: {f1_score:.6f}')\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "ConfusionMatrixDisplay(confusion_matrix=conf_matrix).plot(values_format='')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5YxuM4b0OFcw",
      "metadata": {
        "id": "5YxuM4b0OFcw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}